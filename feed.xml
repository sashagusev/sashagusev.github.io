<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sasha Gusev</title>
    <description>scientist</description>
    <link>http://sashagusev.github.io/</link>
    <atom:link href="http://sashagusev.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 04 Feb 2016 16:59:41 -0500</pubDate>
    <lastBuildDate>Thu, 04 Feb 2016 16:59:41 -0500</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>Quick Takes: Interesting Papers from January</title>
        <description>&lt;p&gt;&lt;em&gt;Intriguing papers that were published in the previous month, with highlights.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;heritability&quot;&gt;Heritability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The contribution of rare variation to prostate cancer heritability, Mancuso et al. Nat Genet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Our finding that 42% (95% confidence interval = 21–63%) of the genetic risk for prostate cancer is due to variants in the MAF range of 0.1–1% is striking, given that only a couple percent of neutral varia- tion is due to SNPs in this frequency range.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abundant contribution of short tandem repeats to gene expression variation in humans, Gymrek et al. Nat Genet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“We used variance partitioning to disentangle the contribution of eSTRs from that of linked SNPs and indels and found that eSTRs contribute 10–15% of the cis heritability [of expression] mediated by all common variants.”&lt;/p&gt;

&lt;p&gt;“We hypothesize that there are more eSTRs to find in the genome…”&lt;/p&gt;

&lt;h2 id=&quot;population-genetics&quot;&gt;Population genetics&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Genomic Signatures of Selective Pressures and Introgression from Archaic Hominids at Human Innate Immunity Genes, Deschamps et al. AJHG&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Using full-genome sequence variation from the 1000 Genomes Project, we first show that innate immunity genes have globally evolved under stronger purifying selection than the remainder of protein-coding genes … Finally, we show that innate immunity genes present higher Neandertal intro- gression than the remainder of the coding genome.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Visualizing spatial population structure with estimated effective migration surfaces, Petkova et al. Nat Genet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“EEMS is a new method for analyzing population structure from geo-referenced genetic samples. EEMS produces an intuitive visual representation of spatial patterns in genetic variation and highlights regions of higher-than-average and lower-than-average historical gene flow.”&lt;/p&gt;

&lt;p&gt;“Distance matrices based on rare SNPs could also provide insights into more recent dispersal history…”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Spatial Framework for Understanding Population Structure and Admixture, Bradbury et al. PLOS Gen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“We use genome-wide polymorphism data to build “geo- genetic maps,” which, when applied to stationary populations, produces a map of the geo- graphic positions of the populations, but with distances distorted to reflect historical rates of gene flow.”&lt;/p&gt;

&lt;p&gt;“Additionally, although we have focused on the covariance among alleles at the same locus, linkage disequilibrium (covariance of alleles among loci) holds rich information about the timing and source of admixture events as well as information about isolation by distance.”&lt;/p&gt;

&lt;p&gt;“The inclusion of ancient DNA samples in the analyzed sample offers a ￼way to get better representation of the ancestral populations from which the ancestors of modern samples received their admixture.”&lt;/p&gt;

&lt;h2 id=&quot;gene-expression&quot;&gt;Gene Expression&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Genetic Variation, Not Cell Type of Origin, Underlies the Majority of Identifiable Regulatory Differences in iPSCs, Burrows et al. PLOS Gen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“We show that the cell type of origin only minimally affects gene expression levels and DNA methylation in iPSCs (induced pluripotent stem cells), and that genetic variation is the main driver of regulatory differences between iPSCs of different donors. Our findings suggest that studies using iPSCs should focus on additional individuals rather than clones from the same individual.”&lt;/p&gt;

&lt;h2 id=&quot;gwas&quot;&gt;GWAS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Leveraging Genomic Annotations and Pleiotropic Enrichment for Improved Replication Rates in Schizophrenia GWAS, Wang et al. PLOS Gen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“We have presented a novel algorithm, called CM3, which provides more accurate estimates of predicted replication probabilities for each SNP in a GWAS. Sorting SNPs based on predicted finite-sample replication probabilities incorporating auxiliary information, rather than by nominal p-values, yields a larger number of SNPs for a given replication threshold.”&lt;/p&gt;

&lt;p&gt;“An important utility of the CM3 method may be selection of a greater proportion of relevant SNPs for gene set enrichment and biological pathway analyses…”&lt;/p&gt;

&lt;h2 id=&quot;big-data&quot;&gt;Big Data&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Genotype Imputation with Millions of Reference Samples, Browning &amp;amp; Browning AJHG&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“We demonstrate that Beagle v.4.1 scales to much larger reference panels [than IMPUTE or Minimac] by performing imputation from a simulated reference panel having 5 million samples”&lt;/p&gt;

&lt;p&gt;“When there are millions of reference samples, use of a binary reference file can reduce wall clock computation time by &amp;gt;80%.”&lt;/p&gt;

&lt;p&gt;“With a reference panel containing 200,000 simulated European individuals, we find that markers with at least nine copies of the minor allele in the reference panel can be imputed with high accuracy (r2 &amp;gt; 0.8) in target samples that have been genotyped with a 1M SNP array.”&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Feb 2016 14:35:00 -0500</pubDate>
        <link>http://sashagusev.github.io/2016-02/papers.html</link>
        <guid isPermaLink="true">http://sashagusev.github.io/2016-02/papers.html</guid>
        
        
        <category>papers</category>
        
      </item>
    
      <item>
        <title>Gaussian process regression</title>
        <description>&lt;p&gt;Gaussian process (GP) regression is an interesting and powerful way of thinking about the old regression problem. Given the standard linear model:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = X\beta + \epsilon&lt;/script&gt;

&lt;p&gt;where we wish to predict values of y in unlabeled test data, a typical solution is to use labeled training data to learn the &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;s (for example, by finding &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;s that minimize normally distributed residuals) and then apply them to test data to make point predictions. The GP instead describes the &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;s as arising from functions that have a joint multivariate-Gaussian distribution and learns the underlying mean and variance parameters. The models can be equally descriptive: just as N observations can be perfectly described by N &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;s, they can be perfectly described by N Gaussians (by centering a Gaussian at each observation and fitting a variance). But the GP offers some interesting additional flexibility, with particular relevance to genetics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;By working in the space of functions instead of &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;s, the GP can express relationships between data and outcome that are intensive or impossible to describe in terms of weights (i.e. infinite functions). The GP defines a prior on the functions (using the kernel) and then allows the available training data to place restrictions on that prior before making predictions. [&lt;em&gt;Aside: this process of restricting the prior and making posterior predictions can also be done iteratively - previous posterior becomes current prior - as more data becomes available.&lt;/em&gt;]&lt;/li&gt;
  &lt;li&gt;By working in the space of observations of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, the GP can efficiently express models where the number of predictors is greater than the number of observations (i.e. more SNPs than samples), which would be intractable for the standard linear model. Of course, there are LM solutions involving penalized regression and these have a close relationship to the GP.&lt;/li&gt;
  &lt;li&gt;Making predictions from a distribution instead of a linear combination of point estimates is conceptually appealing and fits well with the Bayesian formalism. The virtue of being Bayesian, like any matters of religion and politics, are probably not best discussed in a blog so I’ll only add here it also coincides with very attractive figures (see below).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m going through the &lt;a href=&quot;http://www.gaussianprocess.org/gpml/&quot;&gt;Rasmussen and Williams 2006&lt;/a&gt; book on Gaussian Processes for Machine Learning and wanted to outline here some of the basics of working with this model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The mathematical model.&lt;/strong&gt; At the outset I’ll say that most of this will be paraphrasing Rasmussen and Williams (Chapter 2) with some modifications in notation to match what I’m used to. The code in R is minimal (see details at the end), but borrows greatly from &lt;a href=&quot;http://www.jameskeirstead.ca/blog/gaussian-process-regression-with-r/&quot;&gt;this implementation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Continuing from the linear model, we model &lt;script type=&quot;math/tex&quot;&gt;V = cov(y) = K(X,X) + \sigma_n^2 I&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; is the kernel function describing the difference between observations (for simplicity, assume it’s just some N x N covariance over observations in &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;) and &lt;script type=&quot;math/tex&quot;&gt;\sigma_e^2&lt;/script&gt; is the variance on the noise. Since our goal is prediction, unlabeled observations, &lt;script type=&quot;math/tex&quot;&gt;X^*&lt;/script&gt;, can be described using the following joint distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} y \\ y_* \end{bmatrix} \sim N(0, \begin{bmatrix} V &amp; K(X,X_*) \\ K(X_*,X) &amp; K(X_*,X_*) \end{bmatrix} ) %]]&gt;&lt;/script&gt;

&lt;p&gt;The notation is getting heavy but the intuition is that &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; continues to be modeled by &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, and any relationships between &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y_*&lt;/script&gt; are modeled through variations on &lt;script type=&quot;math/tex&quot;&gt;K(X,X_*)&lt;/script&gt;. This leads to the following predictive functions given the training data:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[y_* | X,y,X_* ] = K(X_*,X) V^{-1} y&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;cov( y_* ) = K(X_*,X_*) - K(X_*,X) V^{-1} K(X,X_*)&lt;/script&gt;

&lt;p&gt;Conceptually, the first equation (i.e. the prediction given the data) takes the testing data, pulls it through the relationship to the training data, and then through the relationship between &lt;script type=&quot;math/tex&quot;&gt;V^{-1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, the observed labels. The variance/covariance of the prediction is not as intuitive to me, but an important point is that it only depends on the similarity of the &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;s and not at all on the observations (although &lt;script type=&quot;math/tex&quot;&gt;\sigma^2_e&lt;/script&gt;, which may depend on the observations, is still incorporated). These two simple manipulations of the kernel and variance fully capture prediction from the GP, and are all that is necessary for a working GP prediction (all code described at the end):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;gp_solve = function( x.train , y.train , x.pred , kernel , sigma2e = 0 ) {
	solution = list()
	# compute necessary covariances
	k.xx = kernel(x.train,x.train)
	k.x_xp = kernel(x.train,x.pred)
	k.xp_x = kernel(x.pred,x.train)
	k.xp_xp = kernel(x.pred,x.pred)

	# Invert the covariance matrix with noise
	Vinv = solve(k.xx + sigma2e * diag(1, ncol(k.xx)))
	
	# Compute the prediction
	solution[[&quot;mu&quot;]] = k.xp_x %*% Vinv %*% y.train
	solution[[&quot;var&quot;]] = k.xp_xp - k.xp_x %*% Vinv %*% k.x_xp
	return( solution )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Though I won’t transcribe it here, the fact that everything has been defined in terms of Gaussian distributions means the GP also has an explicit marginal likelihood which empowers us to do formal model comparison and testing. Of note, this is the same likelihood that is used to identify the MLE heritability parameter using GREML.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Gaussian process in practice.&lt;/strong&gt; All of this makes much more sense when looking at how the model handles data. Here, I’ve generated five points from the function &lt;script type=&quot;math/tex&quot;&gt;y = sin(x) + \epsilon&lt;/script&gt; and fit a prediction to these points using the GP. I’ll underscore again that in contrast to the standard (weight-space) regression, I never need to define a linear relationship between the observations &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; and some combination of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;s (i.e. I’m not defining &lt;script type=&quot;math/tex&quot;&gt;y=sin(x) \beta + \epsilon&lt;/script&gt; and looking for a &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;). Instead, I’ve selected a kernel which places a prior on the way observations are related in space (their covariance), allowed the data to constrain the prior, and sampled from the posterior. With more data I get better estimates of the multivariate-Gaussian distribution describing these points, but I don’t explicitly learn the underlying generative function. Below I’ve plotted the mean and confidence interval (2 times the sd) of the GP distribution learned from this data in blue line and blue shading:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/plot_gp.svg&quot; alt=&quot;Gaussian Process Regression&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[Aside: One subtle point that confused me is that the blue fit is not actually being plotted from the generative function with noise the way it is in a standard linear model. Rather, these lines represent draws of predicted values and their corresponding precision from the GP. This underscores the fact that the GP can model functions with infinite weights that are impossible to infer directly.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This figure illustrates an attractive practical consequence of the GP estimating the posterior distribution: flexible confidence intervals. We don’t get just a uniform band around the mean prediction, but intervals that vary with the data. This is specifically demonstrated by the three points to the right of origin, which place a much stronger constraint on the posterior than the two points to the left. An alternative way of seeing this is that there are many more consistent functions that lie between the points on the left. This also leads to a natural way for an online algorithm to sample future points from the parts of the space that are most uncertain (for example, in the case where generating observations is very expensive).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernels&lt;/strong&gt;. Up until this point I’ve avoided talking about the &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;s that are used in the GP. These functions define how the &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;s are related to one another and have to follow certain distance-like rules: they must be symmetric; identical &lt;script type=&quot;math/tex&quot;&gt;X1,X2&lt;/script&gt; values must have &lt;script type=&quot;math/tex&quot;&gt;K(X1,X2) = 0&lt;/script&gt;; see more rules in &lt;a href=&quot;http://gpss.cc/gpss15/talks/KernelDesign.pdf&quot;&gt;this talk&lt;/a&gt;. The kernels describe how nearby points contribute to the covariance of the outcomes, and they allow the same underlying GP machinery to flexibly use different data priors. Again, the best way to understand this is to see the output, so I’ve defined a few kernels:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;# this is pseudocode, see iteration in code at the end

# Exponential kernel
Sigma[i,j] &amp;lt;- exp(-1*(abs(x1[i]-x2[j])))

# Brownian kernel
Sigma[i,j] &amp;lt;- min(x1[i],x2[j])

# Gaussian kernel
Sigma[i,j] &amp;lt;- exp(-0.5*(x1[i]-x2[j])^2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that each kernel is a very simple function that relates pairs of points. Below, I’ve fit these kernels to the previous points and plotted the results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/plot_gp_multi_sin.svg&quot; alt=&quot;GP fit with multiple kernels&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are a few interesting observations here. First, the trusty univariate regression can be captured in the GP using a linear kernel.&lt;/p&gt;

&lt;p&gt;[&lt;em&gt;Aside: One may assume that the linear kernel will still have the pretty non-linear confidence intervals, but the shape of the variance follows the shape of the kernel. I imagine there can be situations where the generative model is linear, but points are drawn non-uniformly from X and the Gaussian kernel is preferred for more flexible confidence intervals.&lt;/em&gt;]&lt;/p&gt;

&lt;p&gt;Second, different kernels can yield very different distributions but tend to follow the principle that more data == more precision. Third, the kernel that’s closest to the generative model - in this case &lt;script type=&quot;math/tex&quot;&gt;sin(x)&lt;/script&gt; - fits the data best.&lt;/p&gt;

&lt;p&gt;Let’s see how each kernel deals with data that is instead sampled from a linear function &lt;script type=&quot;math/tex&quot;&gt;y = x + \epsilon&lt;/script&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/plot_gp_multi_linear.svg&quot; alt=&quot;GP fit of linear data with multiple kernels&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Somewhat surprisingly, the complex kernels still do a pretty good job of fitting the linear function in parts of the space that have labelled observations. On the other hand, where data has not been observed (-4 &amp;gt; X &amp;gt; 4) the confidence intervals quickly expand and the prediction reverts to the prior implied by the kernel. This is even mildly true for the linear kernel, which raises an important point that the GP is not a magic bullet. If we knew that the data was really coming from a simple linear function of X, then the boring OLS - with it’s frumpy uniform confidence intervals - is exactly what we would want. That would give us great, confident predictions at x = 1,000 where the linear GP has never seen labelled data and would be highly uncertain. This is probably obvious, but model flexibility is only valuable if the underlying data necessitates it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Closing thoughts&lt;/strong&gt;. I’m still learning and this is all just scratching the surface of what can be done with the Gaussian process framework. The underlying model can be extended to have multiple outcomes with specific covariance structure (e.g. time series or correlated phenotypes); multiple GPs placing individual priors on partitions of the feature space; approximations to non-Gaussian outputs (e.g. poisson processes or binary classification); and apparently an entire language of kernel functions. In a future post, I’ll attempt to draw explicit connections between this model and work in genetics and consider relevant extensions.&lt;/p&gt;

&lt;p&gt;I’ve been negligent about citations here, but much good reading on this topic is openly available on the &lt;a href=&quot;www.gaussianprocess.org&quot;&gt;gaussianprocess.org web-site&lt;/a&gt;, the &lt;a href=&quot;http://www.gaussianprocess.org/gpml/chapters/&quot;&gt;GPML book&lt;/a&gt;, and the Gaussian Process Summer Schools &lt;a href=&quot;http://gpss.cc/gpss15/&quot;&gt;lectures&lt;/a&gt;. Useful applied examples are also available with the Python &lt;a href=&quot;http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/index.ipynb&quot;&gt;GPy&lt;/a&gt; and &lt;a href=&quot;http://scikit-learn.org/stable/modules/gaussian_process.html&quot;&gt;scikit - GPML&lt;/a&gt; libraries.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;The full code to solve a GP regression and generate all figures in this post is available in this Gist:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/sashagusev/c9287f488cd65c3ede9e&quot;&gt;https://gist.github.com/sashagusev/c9287f488cd65c3ede9e&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Jan 2016 14:35:00 -0500</pubDate>
        <link>http://sashagusev.github.io/2016-01/GP.html</link>
        <guid isPermaLink="true">http://sashagusev.github.io/2016-01/GP.html</guid>
        
        
        <category>regression</category>
        
      </item>
    
      <item>
        <title>On &#39;Limitations of GCTA...&#39; (stratification)</title>
        <description>&lt;p&gt;In a &lt;a href=&quot;http://sashagusev.github.io/2015-12/notes-on-GCTA.html&quot;&gt;previous post&lt;/a&gt;, I discussed some concerns about the findings reported in &lt;a href=&quot;#Krishna-Kumar:2015&quot;&gt;(Krishna Kumar et al. 2015)&lt;/a&gt; of bias in GREML-based heritability estimates. Contra their conclusion, simulations where model assumptions were met yielded unbiased estimates of heritability and it’s approximated standard error. The discrepancy observed in SKK appears to be due to their comparison of approximated standard errors from a large sample to standard deviations from a small sample. SKK also analyzed a real phenotype (systolic blood pressure, SBP) from the Framingham Heart Study data and drew conclusions about GREML for real data. These results include additional complexities due to linkage disequilibrium (LD) and “genetic stratification”. While both are model violations, they are ubiquitous confounders in human studies and the authors are well-motivated to evaluate their impact. However, I believe the various forms of confounding attributed to genetic stratification by SKK are either expected behavior in the presence of LD, or due to incomplete removal of related individuals. Confounding due to stratification is a subtle concept and I’ll start with some simple simulations demonstrating it’s expected effect on GREML and then justify my interpretation of the SKK findings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stratification can bias GREML estimates.&lt;/strong&gt; The GREML model assumes that an individual’s genetic value (the total contribution of genetics to phenotype) is drawn randomly from the population and the covariance across individuals is modeled by the genetic relatedness matrix (GRM) with random environmental noise. When individuals are related, both assumptions are violated. To demonstrate this, I again started with the simulation framework used in SKK and the previous post: SNPs are randomly generated, assigned random effect-sizes, and used to generate an additive phenotype with heritability = 0.75; then, a GRM is constructed from the SNPs and heritability is estimated using GREML. To model independent, untyped variants, I modified the framework so that only half of the SNPs are included in the GRM, which means an unbiased SNP-heritability estimate should be 0.75/2=0.375. The generative model is &lt;script type=&quot;math/tex&quot;&gt;y=X_{g}\beta_{g} + X_{u}\beta_{u} + e&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;X_g&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_u&lt;/script&gt; are the genotyped and untyped SNPs; and the inferential model is &lt;script type=&quot;math/tex&quot;&gt;y \sim \sigma^2_g X_g X_g&#39;/P + \sigma^2_e I&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Then, extreme relatedness is introduced by replacing one individual at a time with their twin (i.e. duplicating their genetics) and rerunning the generative model + estimation. [Note: the GRM can include twins/duplicates and be singular because REML inverts the non-singular matrix &lt;script type=&quot;math/tex&quot;&gt;V = \sigma^2_g GRM + \sigma^2_e I&lt;/script&gt;]. Importantly, there is no population stratification, all SNPs are completely independent, and there is no explicit shared environment between the twins (the environmental term for them is still random). These are interesting additional confounders, but this simulation is meant to demonstrate bias in the most simple case. The estimate of heritability from 500 simulations and increasing levels of relatedness is plotted below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/SKK_twins_untyped.svg&quot; alt=&quot;heritability estimates with relatedness&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For each relatedness level, the density of heritability estimates is shown with a gray violin plot; the mean and standard deviation is indicated by the solid black point and error bar; and the standard error approximated by GREML is indicated by the small unfilled circles. First, when there is no relatedness (x=0) the estimate of SNP-heritability is unbiased around 0.375 and the approximated standard error matches the observed standard deviation. Second, as relatedness is introduced the estimate becomes upwardly biased and quickly approaches the true underlying heritability. Third, as relatedness is introduced the empirical standard deviation decreases (the violin spread decreases). And finally, the approximate standard error (unfilled circles) becomes slightly biased and underestimates the true variance. Notably, these biases are substantial even with a very small fraction of twins in the data (20-50 out of 1,000 total individuals); with less extreme relatedness the same biases could be achieved with hundreds of samples.&lt;/p&gt;

&lt;p&gt;To confirm that this effect is due to the untyped SNPs and not a hidden confounder, I simulated a phenotype with no untyped SNPs (and SNP-heritability = heritability = 0.5). Here, no bias in the estimate or the standard error is observed, though the variance again decreases with increasing relatedness:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/SKK_twins_typed.svg&quot; alt=&quot;heritability estimates with relatedness (and no untyped SNPs)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So how is it that GREML estimates heritability from SNPs that the GRM never sees? In the generative model, relatedness induces a correlation between the typed and untyped SNPs for the related individuals, which then contaminate the SNP-heritability estimate. Intuitively, twins that share all of chromosome 1 also share all of chromosome 2 (both genetics and phenotype), so estimates from the former implicitly include effects from the latter. For unrelated individuals, chr1 is expected to be uncorrelated from chr2, yielding an estimate only of heritability from typed and correlated variants (i.e. SNP-heritability). Studies that include a mixture of related and unrelated individuals will yield a confounded estimate of “pseudo-heritability”, which is neither informative of total heritability (using only related samples) nor SNP-heritability. In this case, a bigger value is not necessarily better if it no longer has a clear interpretation for the trait. Computing accurate estimates of heritability therefore requires rigorously removing weakly related individuals, usually by pruning GRM values &amp;gt;0.05 or 0.025. While this is a heuristic, it ensures that the correlation between typed and untyped variants is no higher than 0.05^2 (in expectation) and mitigates the bias in practice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Previous work.&lt;/strong&gt; GREML and mixed models were introduced to human genetics as a means of controlling for relatedness. The seminal contribution of &lt;a href=&quot;#Yang:2010&quot;&gt;(Yang et al. 2010)&lt;/a&gt; was to establish the concept of SNP-heritability and demonstrating that, for unrelated individuals, this parameter was no longer a measure of confounding but inherently useful. Since then, computation of SNP-heritability has always come with the caveat that relatedness must be strictly controlled for. For a detailed treatment, see &lt;a href=&quot;#Zaitlen:2013&quot;&gt;(Zaitlen et al. 2013)&lt;/a&gt; which quantified this phenomenon and leveraged a two component model to estimate both SNP- and total heritability simultaneously. As well as &lt;a href=&quot;#Vattikuti:2012&quot;&gt;(Vattikuti, Guo, and Chow 2012)&lt;/a&gt; which restricted to related/unrelated individuals for accurate single-component estimates. More recently, a two component model for relatedness has also been used to improve risk prediction &lt;a href=&quot;#Tucker:2015&quot;&gt;(Tucker et al. 2015)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The SKK findings are likely to be confounded by cryptic relatedness.&lt;/strong&gt; All of this throat-clearing is meant to shed some light on what’s going on in the SKK analyses of real data. The paper estimates SNP-heritability of SBP at 0.263 (se 0.048). I believe this estimate is confounded by cryptic relatedness for the following reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SKK report using 2,698 “unrelated” individuals from the FHS. Previous work using the FHS reported 1,489 individuals at a relatedness cutoff of 0.025 &lt;a href=&quot;#Vattikuti:2012&quot;&gt;(Vattikuti, Guo, and Chow 2012)&lt;/a&gt;; and ~2,000 individuals at a relatedness cutoff of 0.05 &lt;a href=&quot;#Wray:2013&quot;&gt;(Wray et al. 2013)&lt;/a&gt;. Assuming the same data are used, SKK are likely including a large number of individuals related at a level above 0.05.&lt;/li&gt;
  &lt;li&gt;The estimate of 0.263 lies between the &lt;a href=&quot;#Vattikuti:2012&quot;&gt;(Vattikuti, Guo, and Chow 2012)&lt;/a&gt; estimate for SBP of h2g=0.24 and h2=0.30. Though there is a lot of sampling error, this is consistent with simulations showing that cryptic relatedness biases the estimate of h2g towards the true h2.&lt;/li&gt;
  &lt;li&gt;The standard error of 0.048 is much lower than expected for this sample size based on the &lt;a href=&quot;http://cnsgenomics.com/shiny/gctaPower/&quot;&gt;GREML power calculator&lt;/a&gt;, lower even than the &lt;a href=&quot;#Vattikuti:2012&quot;&gt;(Vattikuti, Guo, and Chow 2012)&lt;/a&gt; standard error of 0.05 from 5,647 individuals (standard error tracks roughly with sample size, so the FHS should be &amp;gt;2x higher). This is again consistent with the above simulations showing that cryptic relatedness decreases the estimate variance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since removing relatedness is a pre-requisite for estimating SNP-heritability, and since cryptic relatedness can confound all aspects of the heritability estimate, it is difficult to draw any conclusions from the SKK results in real data that generalize to studies of strictly unrelated individuals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other biases.&lt;/strong&gt; Setting aside the confounding from relatedness, is it possible that the SKK results are still demonstrating real bias? In particular, Fig. 4 and Fig. 7 (reproduced below) show down-sampled estimates that are clearly not centered at the full-sample heritability:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/SKK_fig7.png&quot; alt=&quot;Figure 7 from Krishna-Kumar et al&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the previous post demonstrated how differences in down-sampled variance are not indicative of bias, how to interpret this difference in down-sampled mean?&lt;/p&gt;

&lt;p&gt;First, the decrease in the estimate of total heritability (difference from blue lines) is an expected consequence of down-sampling the SNPs. As SKK derive in eq. 3, the SNP-heritability estimates correspond to the total variance explained by all genotyped SNPs (&lt;script type=&quot;math/tex&quot;&gt;V_g=P \sigma^2&lt;/script&gt;); naturally, restricting the GRM to a random sub-sample of SNPs (thereby reducing P) will result in a lower estimate of heritability. Since this is a real trait, the extent and variance of the reduction will depend on the underlying genetic architecture and has nothing to do with the estimation procedure. This again underscores the important conceptual difference between SNP-heritability and total heritability (which is a parameter that does not depend on SNPs).&lt;/p&gt;

&lt;p&gt;Second, the observation that the per-SNP heritability estimate is &lt;em&gt;higher&lt;/em&gt; after down-sampling may be counter-intuitive, but is an expected consequence of LD between SNPs leading to more “effective” SNPs in the down-sampled data. Consider the case where the GRM is constructed from an increasing number of SNPs. A random sample of 5,000 SNPs is like drawing ~2 SNPs per megabase (on average), so there is very little LD between SNPs in the GRM and the &lt;em&gt;effective&lt;/em&gt; number of independent SNPs, accounting for redundant tagging, is close to their total count. As more SNPs are included in the sample, they are more likely to be in LD with a SNP already present in the GRM, and their contribution to heritability previously tagged: the &lt;em&gt;effective&lt;/em&gt; number of independent SNPs will grow slower than the total count of SNPs (this was separately observed by SKK in the section on saturation). In other words, in any genomic data with LD, down-sampled SNPs will tag more heritability per-SNP than full-sample SNPs, and will therefore have higher per-SNP heritability estimates. The reason this was not observed in the simulation is because it modeled the unique case of independent SNPs. Cryptic relatedness will further exacerbate this effect by inducing additional LD in the related individuals.&lt;/p&gt;

&lt;p&gt;Whether you consider this a bias or not depends on the definition of the underlying parameter. Under the common definition of SNP-heritability as the variance explained by SNPs in the GRM &lt;em&gt;and any SNPs in LD with them&lt;/em&gt;, the observations in Fig. 4, Fig. 7 (above), and general saturation are not an indicator of bias but of correlation between SNPs in the GRM.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Closing thoughts.&lt;/strong&gt; I think the SKK paper makes a compelling case - in theory - that genetic stratification can create extremely large and unstable singular values in the GRM and lead to an overfit to the sample. However, I don’t believe that the empirical findings in real data adequately demonstrate that this bias is a serious concern in practice. Importantly, this bias needs to be demonstrated &lt;em&gt;after&lt;/em&gt; stringently removing cryptic relatedness from the data, which will significantly reduce the distortion in singular values. Aside from cryptic relatedness, confounding of heritability estimates by subtle population structure - and whether principal components are an adequate solution in human populations - has been a hotly debated topic &lt;a href=&quot;#Browning:2011&quot;&gt;(Browning and Browning 2011; Goddard et al. 2011)&lt;/a&gt; and remains an open question, though no such confounding has yet been demonstrated.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Krishna-Kumar:2015&quot;&gt;Krishna Kumar, Siddharth, Marcus W Feldman, David H Rehkopf, and Shripad Tuljapurkar. 2015. “Limitations Of GCTA as a Solution to the Missing Heritability Problem.” &lt;i&gt;Proc Natl Acad Sci U S A&lt;/i&gt;, December. doi:10.1073/pnas.1520109113.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yang:2010&quot;&gt;Yang, Jian, Beben Benyamin, Brian P McEvoy, Scott Gordon, Anjali K Henders, Dale R Nyholt, Pamela A Madden, et al. 2010. “Common SNPs Explain a Large Proportion of the Heritability for Human Height.” &lt;i&gt;Nat Genet&lt;/i&gt; 42 (7): 565–69. doi:10.1038/ng.608.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Zaitlen:2013&quot;&gt;Zaitlen, Noah, Peter Kraft, Nick Patterson, Bogdan Pasaniuc, Gaurav Bhatia, Samuela Pollack, and Alkes L Price. 2013. “Using Extended Genealogy to Estimate Components of Heritability for 23 Quantitative and Dichotomous Traits.” &lt;i&gt;PLoS Genet&lt;/i&gt; 9 (5): e1003520. doi:10.1371/journal.pgen.1003520.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Vattikuti:2012&quot;&gt;Vattikuti, Shashaank, Juen Guo, and Carson C Chow. 2012. “Heritability And Genetic Correlations Explained by Common SNPs for Metabolic Syndrome Traits.” &lt;i&gt;PLoS Genet&lt;/i&gt; 8 (3): e1002637. doi:10.1371/journal.pgen.1002637.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Tucker:2015&quot;&gt;Tucker, George, Po-Ru Loh, Iona M MacLeod, Ben J Hayes, Michael E Goddard, Bonnie Berger, and Alkes L Price. 2015. “Two-Variance-Component Model Improves Genetic Prediction In Family Datasets.” &lt;i&gt;Am J Hum Genet&lt;/i&gt; 97 (5): 677–90. doi:10.1016/j.ajhg.2015.10.002.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Wray:2013&quot;&gt;Wray, Naomi R, Jian Yang, Ben J Hayes, Alkes L Price, Michael E Goddard, and Peter M Visscher. 2013. “Pitfalls Of Predicting Complex Traits from SNPs.” &lt;i&gt;Nat Rev Genet&lt;/i&gt; 14 (7): 507–15. doi:10.1038/nrg3457.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Browning:2011&quot;&gt;Browning, Sharon R, and Brian L Browning. 2011. “Population Structure Can Inflate SNP-Based Heritability Estimates.” &lt;i&gt;Am J Hum Genet&lt;/i&gt; 89 (1): 191–93; author reply 193–95. doi:10.1016/j.ajhg.2011.05.025.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Goddard:2011&quot;&gt;Goddard, Michael E., S. Hong Lee, Jian Yang, Naomi R. Wray, and Peter M. Visscher. 2011. “Response To Browning and Browning.” &lt;i&gt;The American Journal Of Human Genetics&lt;/i&gt; 89 (1). Elsevier: 193–95. doi:10.1016/j.ajhg.2011.05.022.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;h1 id=&quot;simulation-details&quot;&gt;Simulation details&lt;/h1&gt;

&lt;p&gt;The code for this simulation framework has been made available at &lt;a href=&quot;https://github.com/sashagusev/SKK-REML-sim-related&quot;&gt;https://github.com/sashagusev/SKK-REML-sim-related&lt;/a&gt;. It follows very closely with the simulation described in the previous post, please see documentation in that post for details. I have made trivial changes to compute the GRM from a subset of SNPs, and to introduce related individuals. The files are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;func_reml.R : Functions implementing GREML with GCTA-like convergence criteria&lt;/li&gt;
  &lt;li&gt;sim.R : Generates genotypes and the GRM. For computational speed I have reduced the sample size to 1,000 but kept the ratio of samples:SNPs constant to maintain the random matrix properties.&lt;/li&gt;
  &lt;li&gt;run_rel.R : Replaces individuals with twins and computes heritability.&lt;/li&gt;
  &lt;li&gt;plot.R : Generates figure with violin plots, assumes run_rel.out exists and is a concatenation of multiple outputs from run_rel.R&lt;/li&gt;
  &lt;li&gt;run_rel_typed.R / plot_typed.R : Slight changes to the above scripts to remove any untyped SNPs, generating the second figure in this post.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 12:21:14 -0500</pubDate>
        <link>http://sashagusev.github.io/2016-01/notes-on-GCTA2.html</link>
        <guid isPermaLink="true">http://sashagusev.github.io/2016-01/notes-on-GCTA2.html</guid>
        
        
        <category>heritability</category>
        
      </item>
    
      <item>
        <title>On &#39;Limitations of GCTA...&#39;</title>
        <description>&lt;p&gt;The recent paper of &lt;a href=&quot;#Krishna-Kumar:2015&quot;&gt;(Krishna Kumar et al. 2015)&lt;/a&gt; [&lt;a href=&quot;http://www.pnas.org/content/early/2015/12/17/1520109113.full.pdf&quot;&gt;pdf&lt;/a&gt;] makes a provocative claim challenging the GREML model (implemented in the software GCTA), which has been widely used to estimate components of heritability: “&lt;em&gt;Here, we show that GCTA applied to current SNP data cannot produce reliable or stable estimates of heritability&lt;/em&gt;”. This paper continues an important discussion on the robustness of GREML, but I believe the empirical claims about bias and reliability are overstated and should not cast doubt on properly computed GREML estimates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What does GCTA do?&lt;/strong&gt; The SKK paper does a good job of summarizing the assumptions and goals of GCTA. In brief, GCTA uses genetic data from unrelated individuals to estimate the variance in phenotype that can be explained by genotyped SNPs (often called SNP-heritability, or h2g). It assumes a model where phenotypes are drawn from a multi-variate normal distribution with variance modeled by a genetic relatedness matrix (GRM) and an environmental term (the identity matrix). It directly estimates entries in the GRM as the covariance of a pair of individuals over all genotyped SNPs; it uses the REML algorithm to identify the maximum likelihood estimate of the variance attributed to the GRM; and it uses the Hessian of the likelihood to approximate the corresponding standard error. GCTA and related papers are landmarks in genetics, but the criticisms apply to all methods that use this combination of GRM and the REML algorithm, so I will refer to the general approach as GREML to distinguish between model and implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why is this a big deal?&lt;/strong&gt; Geneticists are generally interested in understanding the “genetic architecture” of a trait: the distribution/relationship between trait causing effect-sizes and genetic variants. How much of a trait is due to rare or common variants? How much is due to variants that can be tagged by a genotyping array, or an exome array, or coding SNPs? There are lots of ways to answer these questions, but the primary advantage of the GREML approach is that it provides estimates that are neither biased by sample size nor dependent on individually significant associations. This means that the seminal finding of [&lt;em&gt;Yang et al. 2010 Nature Genetics&lt;/em&gt;] that common variants explain 45% of height is a conclusion about the trait, not about the study, and continues to be relevant to study design in 2015. Hundreds of such analyses have since been carried out, and if the estimates are shown to be unreliable then they can no longer be used to make accurate predictions about the future and lose their value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What do Krishna Kumar et al. show?&lt;/strong&gt; The SKK paper demonstrates a relationship between the GREML likelihood and the singular value decomposition of the GRM, which allows them to make general theoretical claims about the instability of GREML estimates. This is an important advance; GREML is mostly treated as a black-box algorithm for estimating one very important parameter, and more intuition on its stability and expected performance is an important contribution. However, the paper also makes empirical claims about the instability of GREML, concluding that the estimates are biased when all model assumptions are met as well as in analyses of real traits. Here, I believe the paper draws conclusions that are not supported by the data and overstates the instability of the GREML approach. In this post, I will focus on “Case 1” from the paper, where the GREML assumptions are fully satisfied but resulting estimates are purported to be biased.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is bias?&lt;/strong&gt; A biased estimator is one where the estimate does not converge to the underlying population parameter with increasing information. In the case of GREML, this could mean that (A) the h2g estimate is systematically and significantly different from the true h2g in the population; or (B) the estimated standard error is systematically different from the true sampling error of estimate. Scenario A is a big problem for drawing conclusions from current estimates. Scenario B is sub-optimal but is only a big problem if the bias is an underestimate - that is, we think the estimate is more precise than it truly is. The SKK paper does not explicitly state what kind of bias they identify, but “Case 1” only focuses on the standard error of the estimate (specifically: “&lt;em&gt;More than half of these estimates lie outside the 95% confidence interval&lt;/em&gt;”) and therefore I believe their focus is on Scenario B. Here, I re-analyzed the SKK simulations to quantify this bias.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simulations from the SKK model.&lt;/strong&gt; The SKK paper describes a simulation framework that satisfies the GREML assumptions: 50,000 SNPs for 2,000 individuals are randomly generated; SNP effect-sizes are independent and identically distributed (the infinitesimal model); and there is no population structure. I replicated the SKK simulations (see details and code below) and re-plotted their Figure 2. Specifically, I generated 50,000 random SNPs (MAF=0.5); 2,000 individuals; and an infinitesimal phenotype with h2g = 0.75. The GREML estimate of h2g for this sample was 0.60 (se 0.15), within the noise of the SKK simulation estimate of 0.69 (se 0.15). I then performed 500 down-samples of the data to 5,000 random SNPs, recomputed the GRM, and re-estimated h2g. Following the SKK format, I plot the overall estimated h2g per-SNP (0.69/50,000) and the density of the down-sampled h2g per-SNP (down-sampled h2g/5,000) below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/plot_krishna_kumar_pnas.svg&quot; alt=&quot;down-sampled heritability estimates&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, the down-sampled estimates are roughly symmetric and centered at the full-sample estimate (solid line in the middle). The average down-sampled estimate is 1.17e-5 (se 4.0e-7) and not significantly different from the full-sample estimate of 1.21e-5. &lt;strong&gt;Therefore, the GREML estimate appears to be unbiased in the SKK simulation.&lt;/strong&gt; Second, each down-sampled GREML estimate also computes an approximate 95% confidence interval, and I have plotted that (averaged over 500 runs) with dotted lines. This estimated confidence interval (CI) is wider than the empirical 95% interval of down-sampled estimates (blue shaded region). &lt;strong&gt;Therefore, the GREML confidence interval appears to be accurate or mildly conservative in the SKK simulation.&lt;/strong&gt; The standard error is a measure of the expected sampling standard deviation of the estimate, or (for an unbiased estimator) the interval where the true underlying parameter is expected to lie - both of which are confirmed to be accurate in these simulations.&lt;/p&gt;

&lt;p&gt;Finally, the estimated confidence interval from all 50,000 SNPs is shown in dashed lines. This is the estimate the SKK paper finds to be much lower than the down-sampled variance (red arrows in SKK Fig. 2), and that observation is confirmed here. There is no justification provided for why the estimated CI from a 50k SNP GRM should be compared to the variance of estimates from a 5k SNP GRM. Both the MLE and the likelihood surface is specific to the genetic data used for estimation and make no guarantees about sub-sampled data. Indeed, this very point motivates the SKK paper: “The MLEs produced by GCTA depend on the properties of the GRM matrix”. In the context of linear regression (where the dual of the GRM is used) the SKK analysis is like comparing the standard error of a regression coefficient computed in 50k individuals to the variance in the estimate computed in random 5k sub-groups. The down-sampling strategy proposed by SKK is therefore not an assessment of bias, and re-analysis of their simulations here shows that the GREML estimates are indeed unbiased.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other work&lt;/strong&gt;. Naturally, the first hurdle a method should clear is that it works under the model assumptions. Several independent analyses have shown this to be the case for GREML, including the work of &lt;a href=&quot;#Speed:2012&quot;&gt;(Speed et al. 2012)&lt;/a&gt; (see Fig.1 “ALL”) and &lt;a href=&quot;#Zhou:2013&quot;&gt;(Zhou, Carbonetto, and Stephens 2013)&lt;/a&gt; (see Fig.1B,D). Importantly, Speed et al. elegantly showed how &lt;em&gt;violations&lt;/em&gt; to the model assumptions on LD can lead to bias, and producing an optimal solution continues to be an open challenge. Lastly, the developers of GCTA have recently released a &lt;a href=&quot;http://cnsgenomics.com/shiny/gctaPower/&quot;&gt;statistical power calculator&lt;/a&gt; to compute the expected standard error of h2g given sample parameters. For a sample size of 2,000, h2g of 0.75, and default GRM variance (2e-5) the expected standard error is 0.15 and matches what was observed in the full-sample simulations. For a sample size of 2,000, h2g of 0.75/10 and down-sampled GRM variance of 10*2e-5, the expected standard error is 0.05, also matching what was observed in the down-sampled simulations. In general, GREML bias under model violations (see also &lt;a href=&quot;#Golan:2014&quot;&gt;(Golan, Lander, and Rosset 2014)&lt;/a&gt;) is an important area of research, and the unique perspective provided by the SKK paper is useful. But the method has been thoroughly reproduced when model assumptions are satisfied and the results of “Case 1” in &lt;a href=&quot;#Krishna-Kumar:2015&quot;&gt;(Krishna Kumar et al. 2015)&lt;/a&gt; do not cast doubt on previous simulations and theory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Krishna-Kumar:2015&quot;&gt;Krishna Kumar, Siddharth, Marcus W Feldman, David H Rehkopf, and Shripad Tuljapurkar. 2015. “Limitations Of GCTA as a Solution to the Missing Heritability Problem.” &lt;i&gt;Proc Natl Acad Sci U S A&lt;/i&gt;, December. doi:10.1073/pnas.1520109113.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Speed:2012&quot;&gt;Speed, Doug, Gibran Hemani, Michael Johnson, and David Balding. 2012. “Improved Heritability Estimation From Genome-Wide SNPs.” &lt;i&gt;Am J Hum Genet&lt;/i&gt; 91 (6): 1011–21. doi:10.1016/j.ajhg.2012.10.010.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Zhou:2013&quot;&gt;Zhou, Xiang, Peter Carbonetto, and Matthew Stephens. 2013. “Polygenic Modeling With Bayesian Sparse Linear Mixed Models.” &lt;i&gt;PLoS Genet&lt;/i&gt; 9 (2). Public Library of Science: e1003264. doi:10.1371/journal.pgen.1003264.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Golan:2014&quot;&gt;Golan, David, Eric Lander, and Saharon Rosset. 2014. “Measuring Missing Heritability: Inferring the Contribution of Common Variants.” &lt;i&gt;Proceedings Of the National Academy of Sciences&lt;/i&gt; 111 (49): E5272–E5281. doi:10.1073/pnas.1419064111.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The SKK paper contains a typo on pg.3, which states that the MLE has SD 1.51/50,000 = 3.1e-6. In fact, 1.51/50,000 = 3.1e-5, which would have fully covered the variance they observed. I assumed the intended computation is 0.151/50,000 = 3.0e-6, which is consistent with the computation on pg.4.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The SKK paper correctly notes that GREML assumes each SNP to have a random, i.i.d effect-size, but their simulation framework includes 5,000 non-causal SNPs which violates this assumption. For clarity, I’ve simulated every SNP to be causal, but in practice this assumption does not matter much unless the fraction of causal variants is very low. Below I’ve run a separate simulation where 10% of the SNPs are causal (more realistic and aggressive than the SKK set-up where 90% are causal) and the previous conclusions hold. See Speed et al. and Zhou et al. for additional torture tests.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://sashagusev.github.io/images/plot_krishna_kumar_pnas.noninf.svg&quot; alt=&quot;down-sampled heritability estimates&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fig. 2 of the SKK paper is bounded at zero. While a variance estimate &amp;lt;0 in real data is un-intuitive, it is important to allow the MLE to go below zero when assessing bias in simulations. In the simplest case where the truth is zero, an estimator that’s bounded at zero will always appear biased. This likely explains the asymmetric distribution observed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;simulation-details&quot;&gt;Simulation details&lt;/h1&gt;

&lt;p&gt;The code for this simulation framework has been made available at &lt;a href=&quot;https://github.com/sashagusev/SKK-REML-sim&quot;&gt;https://github.com/sashagusev/SKK-REML-sim&lt;/a&gt;. The simulation framework is written in R and divided into three main components which rely on GREML functions I have implemented in the &lt;code&gt;func_reml.R&lt;/code&gt; file. These functions use GCTA-like convergence criteria, and should give nearly identical outputs to running GCTA directly. The workflow is: (1) Generate the full-sample genotypes, GRM, and phenotype and estimate full-sample h2g; (2) Load the data from (1), down-sample to 5,000 SNPs and re-estimate heritability 500 times; (3) Collate and plot the results.&lt;/p&gt;

&lt;h2 id=&quot;full-sample-simulation&quot;&gt;(1) Full-sample simulation&lt;/h2&gt;

&lt;p&gt;Code to simulate the genotypes and estimate heritability is in the file &lt;code&gt;sim.R&lt;/code&gt; and assumes &lt;code&gt;func_reml.R&lt;/code&gt; exists in the same directory. This script will generate a “sim.rbin” file which is necessary for step (2). The code is outlined below.&lt;/p&gt;

&lt;p&gt;Load GREML functions and define the parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;source(&#39;func_reml.R&#39;)

# reproducibility!
set.seed(1234)

N = 2000
P = 50000
h2 = 0.75
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Generate genotype matrix with MAF = 0.5, and kinship:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;Z = matrix(rbinom(N*P,2,0.5),nrow=N,ncol=P)

# standardize
for ( i in 1:P ) {
Z[,i] = (Z[,i] - mean(Z[,i]))/sd(Z[,i])
if ( i %% 1000 == 0 ) cat(i,&#39;\n&#39;,file=stderr())
}

# generate kinship
A = Z %*% t(Z) / P
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Generate an infinitesimal, heritable phenotype:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;# 1. SNP effects
u = rnorm(P,0,1)
# 2. genetic value
g = Z %*% u
g = (g - mean(g))/sd(g)
# 3. add environmental noise
y = sqrt(h2) * g + rnorm(N,0,sqrt(1-h2))
y = (y - mean(y))/sd(y)

# save data
save(N,P,h2,y,g,u,A,Z,file=&quot;sim.rbin&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estimate the h2g of the phenotype using GREML:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;# func_reml requires a list of GRMs
K = list()
K[[1]] = A
# see func_reml.R for details, this runs GCTA-style GREML
reml = aiML(K,y,c(0.5,0.5))
cat( &quot;estimate&quot; , reml$h2 , reml$se[1] , &#39;\n&#39; )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Final output is &lt;code&gt;&quot;estimate 0.603424 0.150628&quot;&lt;/code&gt; and is also in the file &lt;code&gt;sim.out&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;down-sample-simulation&quot;&gt;(2) Down-sample simulation&lt;/h2&gt;

&lt;p&gt;Code to down-sample the data and re-estimate heritability is in &lt;code&gt;run_random.R&lt;/code&gt;, which must be run using the command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;R --slave --args $SEED &amp;lt; run_random.R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The concatenated output from 500 runs (with seeds 1-500) is in the file &lt;code&gt;run_random.out&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;plotting&quot;&gt;(3) Plotting&lt;/h2&gt;

&lt;p&gt;Code to generate the above figures is in &lt;code&gt;plot.R&lt;/code&gt; which assumes &lt;code&gt;sim.out&lt;/code&gt; and &lt;code&gt;run_random.out&lt;/code&gt; both exist.&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Dec 2015 12:21:14 -0500</pubDate>
        <link>http://sashagusev.github.io/2015-12/notes-on-GCTA.html</link>
        <guid isPermaLink="true">http://sashagusev.github.io/2015-12/notes-on-GCTA.html</guid>
        
        
        <category>heritability</category>
        
      </item>
    
  </channel>
</rss>
